{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW TO USE OPTICHILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING THE NECESSARY MODULES TO RUN THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "# code to add to import from optichill folder\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from optichill import bas_filter\n",
    "from optichill import GBM_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTERING OUT THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `bas_filter.train_single_plant` to allow the data to import the data, filter out features that are redundent and alarms to provide a training and testing dataset that can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spitting the data from Plant 1 to training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    'Plt1 m 2016-11.csv', 'Plt1 m 2016-12.csv', 'Plt1 m 2017-01.csv', 'Plt1 m 2017-02.csv',\n",
    "    'Plt1 m 2017-03.csv'\n",
    "]\n",
    "test_data = [ \n",
    "    'Plt1 m 2017-04.csv', 'Plt1 m 2017-05.csv', 'Plt1 m 2017-06.csv', 'Plt1 m 2017-07.csv', \n",
    "    'Plt1 m 2017-08.csv', 'Plt1 m 2017-09.csv', 'Plt1 m 2017-10.csv', 'Plt1 m 2017-11.csv', \n",
    "    'Plt1 m 2017-12.csv', 'Plt1 m 2018-01.csv', 'Plt1 m 2018-02.csv', 'Plt1 m 2018-03.csv', \n",
    "    'Plt1 m 2018-04.csv' \n",
    "]\n",
    "\n",
    "points_list = '../../Plt1/Plt1 Points List.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two dataframes (training and testing) are obtained using the `train_single_plant` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Training Set\n",
      "['../../Plt1\\\\Plt1 m 2016-11.csv']\n",
      "['../../Plt1\\\\Plt1 m 2016-12.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-01.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-02.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-03.csv']\n",
      "Descriptors in the points list that are not in the datasets.\n",
      "CommunicationFailure_COV\n",
      "CH3COM1F\n",
      "CH3Ready\n",
      "CH4COM1F\n",
      "CH4Ready\n",
      "CH4SURGE\n",
      "CH5COM1F\n",
      "CH5Ready\n",
      "Original data contains 40396 points and 413 dimensions.\n",
      "Filtered data contains 35940 points and 193 dimensions.\n",
      "Filtering Test Set\n",
      "['../../Plt1\\\\Plt1 m 2017-04.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-05.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-06.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-07.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-08.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-09.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-10.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-11.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-12.csv']\n",
      "['../../Plt1\\\\Plt1 m 2018-01.csv']\n",
      "['../../Plt1\\\\Plt1 m 2018-02.csv']\n",
      "['../../Plt1\\\\Plt1 m 2018-03.csv']\n",
      "['../../Plt1\\\\Plt1 m 2018-04.csv']\n",
      "Descriptors in the points list that are not in the datasets.\n",
      "CommunicationFailure_COV\n",
      "CH3COM1F\n",
      "CH3Ready\n",
      "CH4COM1F\n",
      "CH4Ready\n",
      "CH4SURGE\n",
      "CH5COM1F\n",
      "CH5Ready\n",
      "Original data contains 107168 points and 414 dimensions.\n",
      "Filtered data contains 104167 points and 193 dimensions.\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = bas_filter.train_single_plt(\n",
    "    '../../Plt1', train_data, test_data, points_list, \n",
    "    include_alarms = True, dim_remove = []\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a datasest with kW/Ton and all the other features. This is similar to splitting the data into \"x and y\"\n",
    "axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = df_train['kW/Ton']\n",
    "ytest = df_test['kW/Ton']\n",
    "xtrain = df_train.drop(['kW/Ton'], axis=1)\n",
    "xtest = df_test.drop(['kW/Ton'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING GBM (GRADIENT BOOSTING MACHINES) FOR DETERMINING FEATURE IMPORTANCE AND PREDICTING EFFICIENCY "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model by inputting the \"x and y\" datasets and using the `GBM_model.train_model` function. The R<sup>2</sup> gets printed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8853841244359621"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBM_model.train_model(xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.01, loss='ls', max_depth=6, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=500, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBM_model.predict_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the features importance list into a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature importance list was created as Plt1.csv\n"
     ]
    }
   ],
   "source": [
    "GBM_model.feature_importance_list('Plt1.csv', xtest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
