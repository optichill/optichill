{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_and_filter(dat_folder, string, keys):\n",
    "    \"\"\"imports plant data and creates data frames with filtered data and keys\n",
    "    \n",
    "    Input:\n",
    "    dat_folder = folder containing raw data.\n",
    "    string = prefix of the csv files to be imported.\n",
    "    keys = file name from current directory containing the keys spreadsheet\n",
    "\n",
    "    Output:\n",
    "    bas1 = dataframe containing filtered plant data\n",
    "    key = dataframe containing descriptor key\"\"\"\n",
    "\n",
    "    df, key = data_import(dat_folder, string, keys)\n",
    "    df1 = time_filter(df, key, time_list)\n",
    "    bas = data_BAS(df1, key, key_list, dim_remove=[])\n",
    "    bas1 = alarm_filter(bas, key)\n",
    "    return bas1, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(dat_folder, string, keys):\n",
    "    \"\"\"imports plant data and creates data frames with raw data and keys\n",
    "\n",
    "    dat_folder = folder containing raw data.\n",
    "    string = prefix of the csv files to be imported.\n",
    "    keys = file name from current directory containing the keys spreadsheet\n",
    "\n",
    "    Output:\n",
    "    df = dataframe containing plant data\n",
    "    key = dataframe containing descriptor key\"\"\"\n",
    "\n",
    "    # Assert that dat_folder is .csv\n",
    "\n",
    "    # Assert that string is string type\n",
    "\n",
    "    #extracts file names\n",
    "    dat_list = [f for f in glob.glob(os.path.join(dat_folder, string + '*'))]\n",
    "    print(dat_list)\n",
    "    \n",
    "    #reads and appends content from file to a data frame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for lst in dat_list:\n",
    "        df_add = pd.read_csv(lst)\n",
    "        df = pd.concat([df, df_add], ignore_index=True)\n",
    "    \n",
    "    key = pd.read_excel(keys)\n",
    "    \n",
    "    return df, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alarm_filter(bas, key):\n",
    "\t\"\"\"removes any datapoints with alarms going off or without optimum control\n",
    "\n",
    "\tbas = dataframe containing plant data\n",
    "    key = dataframe containing descriptor key\"\"\"\n",
    "\n",
    "    #filters kes to select those with alarm units that are also BAS\t\n",
    "\tkey_alarm = key[key['Units'].str.contains(\"Normal/Alarm\")==True]\n",
    "\tvals = [x for x in key_alarm if x in bas.columns]\n",
    "\n",
    "\tfor alm in vals:\n",
    "\t\tbas = bas[bas[alm] == 0]\n",
    "\n",
    "\tbas = bas[bas['OptimumControl'] == 1]\n",
    "\n",
    "\treturn bas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated `data_BAS` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = ['BAS', 'Chiller', 'Condenser Water Pump', 'Cooling Tower Cell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_BAS(df, key, key_list, dim_remove=[]):\n",
    "    '''Filters out non-BAS descriptors and data containing NaN values\n",
    "\n",
    "    df = dataframe containing plant data\n",
    "    key = dataframe containing descriptor key'''\n",
    "    \n",
    "    keys = []\n",
    "    kk = []\n",
    "    val = []\n",
    "   \n",
    "    for k in range(0, len(key_list)):\n",
    "        keys.append(key.loc[key['PointType'].str.contains(key_list[k])==True, 'DataPointName'])\n",
    "        kk.append(keys[k].values.tolist())\n",
    "        val += kk[k]\n",
    "    #key_bas = key.loc[key['PointType'].str.contains(\"BAS\")==True,'DataPointName']\n",
    "\t#key_chiller = key.loc[key['PointType'].str.contains(\"Chiller\")==True,'DataPointName']\n",
    "\t#key_condenser = key.loc[key['PointType'].str.contains(\"Condenser Water Pump\")==True,'DataPointName']\n",
    "\t#key_cool = key.loc[key['PointType'].str.contains(\"Cooling Tower Cell\")==True,'DataPointName']\n",
    "\t\n",
    "    #key = pd.concat([key_bas, key_condenser, key_cool, key_chiller], ignore_index = True)\n",
    "\t#print(key.head())\n",
    "\t#converts pandas series to a list for future use\n",
    "\n",
    "    #removes DataPointNames that containt the prefix CHWV\n",
    "    kw = [x for x in val if not 'kW' in x]\n",
    "    vals = [x for x in kw if not x.startswith('CHWV')]\n",
    "\n",
    "    #tests whether all values from the point list spreadsheet are column headings of the dataset\n",
    "    for x in vals:\n",
    "        if x not in df.columns:\n",
    "            #prints and removes any string not found in the data\n",
    "            print(x)\n",
    "            vals.remove(x)\n",
    "        #tests whether all values from the point list spreadsheet are column headings of the dataset\n",
    "\n",
    "    vals_new = [x for x in vals if x in df.columns]\n",
    "\t#vals_kw = [x for x in vals_new if not x]\n",
    "\t#print(vals_new)\n",
    "\t\n",
    "\t#for x in df.columns:\n",
    "\t\t#if x not in vals:\n",
    "            #prints and removes any string not found in the data\n",
    "\t\t\t#print(x)\n",
    "    #expresses data using columns specified by the vals list\n",
    "    bas = df[vals_new+['OptimumControl', 'kW/Ton']]\n",
    "    \n",
    "    print('Original data contains '+str(df.shape[0])+' points and '+str(df.shape[1])+ ' dimensions.')\n",
    "    print('Filtered data contains '+str(bas.dropna().shape[0])+' points and '+str(bas.dropna().shape[1])+ ' dimensions.')\n",
    "    return bas.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to remove data of a range of timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = ['2017-06-07', '2017-06-08', '2017-06-09', '2017-06-10', \n",
    "             '2017-06-11', '2017-06-12', '2017-06-13', '2017-06-14',\n",
    "             '2017-06-15', '2017-06-16', '2017-06-17', '2017-06-18', \n",
    "             '2017-06-19', '2017-06-20', '2017-06-21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_filter(df, key, time_list):\n",
    "    ''' Filters out a specified timestamp from the dataset \n",
    "    \n",
    "    df = dataframe containing the plant data\n",
    "    key = dataframe containing descriptor key\n",
    "    time_list = timestamps to be removed'''\n",
    "    \n",
    "    df = df[~df['timestamp'].str.contains('|'.join(time_list))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../Plt1\\\\Plt1 m 2016-11.csv', '../../../Plt1\\\\Plt1 m 2016-12.csv', '../../../Plt1\\\\Plt1 m 2017-01.csv', '../../../Plt1\\\\Plt1 m 2017-02.csv', '../../../Plt1\\\\Plt1 m 2017-03.csv', '../../../Plt1\\\\Plt1 m 2017-04.csv', '../../../Plt1\\\\Plt1 m 2017-05.csv', '../../../Plt1\\\\Plt1 m 2017-06.csv', '../../../Plt1\\\\Plt1 m 2017-07.csv', '../../../Plt1\\\\Plt1 m 2017-08.csv', '../../../Plt1\\\\Plt1 m 2017-09.csv', '../../../Plt1\\\\Plt1 m 2017-10.csv', '../../../Plt1\\\\Plt1 m 2017-12.csv', '../../../Plt1\\\\Plt1 m 2018-01.csv', '../../../Plt1\\\\Plt1 m 2018-02.csv', '../../../Plt1\\\\Plt1 m 2018-03.csv', '../../../Plt1\\\\Plt1 m 2018-04.csv']\n",
      "CommunicationFailure_COV\n",
      "CH3COM1F\n",
      "CH3Ready\n",
      "CH4COM1F\n",
      "CH4Ready\n",
      "CH4SURGE\n",
      "CH5COM1F\n",
      "CH5Ready\n",
      "Original data contains 138923 points and 414 dimensions.\n",
      "Filtered data contains 131597 points and 193 dimensions.\n"
     ]
    }
   ],
   "source": [
    "df, key = import_and_filter('../../../Plt1', 'Plt1 m', '../../../Plt1/Plt1 Points List.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
