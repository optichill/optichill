{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from optichill import bas_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated `data_BAS` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = ['BAS', 'Chiller', 'Condenser Water Pump', 'Cooling Tower Cell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_BAS(df, key, key_list, dim_remove=[]):\n",
    "    '''Filters out non-BAS descriptors and data containing NaN values\n",
    "\n",
    "    df = dataframe containing plant data\n",
    "    key = dataframe containing descriptor key'''\n",
    "    \n",
    "    keys = []\n",
    "    kk = []\n",
    "    val = []\n",
    "   \n",
    "    for k in range(0, len(key_list)):\n",
    "        keys.append(key.loc[key['PointType'].str.contains(key_list[k])==True, 'DataPointName'])\n",
    "        kk.append(keys[k].values.tolist())\n",
    "        val += kk[k]\n",
    "    #key_bas = key.loc[key['PointType'].str.contains(\"BAS\")==True,'DataPointName']\n",
    "\t#key_chiller = key.loc[key['PointType'].str.contains(\"Chiller\")==True,'DataPointName']\n",
    "\t#key_condenser = key.loc[key['PointType'].str.contains(\"Condenser Water Pump\")==True,'DataPointName']\n",
    "\t#key_cool = key.loc[key['PointType'].str.contains(\"Cooling Tower Cell\")==True,'DataPointName']\n",
    "\t\n",
    "    #key = pd.concat([key_bas, key_condenser, key_cool, key_chiller], ignore_index = True)\n",
    "\t#print(key.head())\n",
    "\t#converts pandas series to a list for future use\n",
    "\n",
    "    #removes DataPointNames that containt the prefix CHWV\n",
    "    kw = [x for x in val if not 'kW' in x]\n",
    "    vals = [x for x in kw if not x.startswith('CHWV')]\n",
    "\n",
    "    #tests whether all values from the point list spreadsheet are column headings of the dataset\n",
    "    for x in vals:\n",
    "        if x not in df.columns:\n",
    "            #prints and removes any string not found in the data\n",
    "            print(x)\n",
    "            vals.remove(x)\n",
    "        #tests whether all values from the point list spreadsheet are column headings of the dataset\n",
    "\n",
    "    vals_new = [x for x in vals if x in df.columns]\n",
    "\t#vals_kw = [x for x in vals_new if not x]\n",
    "\t#print(vals_new)\n",
    "\t\n",
    "\t#for x in df.columns:\n",
    "\t\t#if x not in vals:\n",
    "            #prints and removes any string not found in the data\n",
    "\t\t\t#print(x)\n",
    "    #expresses data using columns specified by the vals list\n",
    "    bas = df[vals_new+['OptimumControl', 'kW/Ton']]\n",
    "    \n",
    "    print('Original data contains '+str(df.shape[0])+' points and '+str(df.shape[1])+ ' dimensions.')\n",
    "    print('Filtered data contains '+str(bas.dropna().shape[0])+' points and '+str(bas.dropna().shape[1])+ ' dimensions.')\n",
    "    return bas.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to remove data of a range of timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_filter(df, start, end):\n",
    "    ''' Filters out a specified timestamp from the dataset \n",
    "    \n",
    "    df = dataframe containing the plant data\n",
    "    key = dataframe containing descriptor key\n",
    "    time_list = timestamps to be removed'''\n",
    "    \n",
    "    time_list = []\n",
    "    date1 = '2017-06-06'\n",
    "    date2 = '2017-06-20'\n",
    "    start = datetime.datetime.strptime(date1, '%Y-%m-%d')\n",
    "    end = datetime.datetime.strptime(date2, '%Y-%m-%d')\n",
    "    step = datetime.timedelta(days=1)\n",
    "    while start <= end:\n",
    "        start += step\n",
    "        time_list.append(start.date())\n",
    "\n",
    "\n",
    "    df = df[~df['timestamp'].str.contains('|'.join(time_list))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bas_filter.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Overall Function  #\n",
    "\n",
    "def train_single_plt(\n",
    "    folder, train_filenames, test_filenames,\n",
    "    keys, start, end, include_alarms=True, dim_remove=[], \n",
    "):\n",
    "    \n",
    "    \"\"\"imports test and train plant data and creates data frames with filtered data\n",
    "    Input:\n",
    "    folder = path to location of raw data files\n",
    "    train_filenames = list of filenames to be used as the training set.\n",
    "    test_filenames = list of filenames to be used as the testing set.\n",
    "    keys = file name from current directory containing the keys spreadsheet\n",
    "    key_list = list of categories to be included in the dataset\n",
    "    include_alarms = include or remove alarms from dataset (default = True)\n",
    "    dim_remove = list of descriptors to remove from dataset (default = NULL)\n",
    "    start = start date of the time frame of data to be removed\n",
    "    end = end date of data to be removed\n",
    "\n",
    "    Output:\n",
    "    df_bas1_train = dataframe containing filtered training data\n",
    "    df_bas1_test = dataframe containing filtered testS data\"\"\"\n",
    "    print('Filtering Training Set')\n",
    "\n",
    "    bas1_train = import_and_filter(\n",
    "        folder, train_filenames, keys, start, end, \n",
    "        include_alarms=include_alarms, dim_remove=dim_remove\n",
    "    )\n",
    "\n",
    "    print('Filtering Test Set')\n",
    "\n",
    "    bas1_test = import_and_filter(\n",
    "        folder, test_filenames, keys, start, end, \n",
    "        include_alarms=include_alarms, dim_remove=dim_remove\n",
    "    )\n",
    "\n",
    "    # matches training and testing columns\n",
    "    vals_test = [x for x in bas1_test.columns if x in bas1_train.columns]\n",
    "    df_bas1_test = bas1_test[vals_test]\n",
    "    df_bas1_train = bas1_train[vals_test]\n",
    "\n",
    "    return df_bas1_train, df_bas1_test\n",
    "\n",
    "#  Sub Components  #\n",
    "\n",
    "def import_and_filter(\n",
    "    folder, train_filenames, keys, start, end, include_alarms=True, dim_remove=[]\n",
    "):\n",
    "    \"\"\"imports plant data and creates data frames with filtered data and keys\n",
    "\n",
    "    Input:\n",
    "    folder = folder containing raw data.\n",
    "    train_filenames = list of filenames to be used as dataset.\n",
    "    keys = file name from current directory containing the keys spreadsheet\n",
    "    include_alarms = include or remove alarms from dataset (default = True)\n",
    "    dim_remove = list of descriptors to remove from dataset (default = NULL)\n",
    "\n",
    "    Output:\n",
    "    bas1 = dataframe containing filtered plant data\"\"\"\n",
    "\n",
    "    # creates blank dataframe for appending\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # imports and concats all datasets\n",
    "    for train_string in train_filenames:\n",
    "\n",
    "        dfloop, key = data_import(\n",
    "            folder, train_string, keys\n",
    "        )\n",
    "        df = pd.concat([df, dfloop], ignore_index=True)\n",
    "\n",
    "    # removes optional timeframes\n",
    "    df_time = time_filter(df, start, end)\n",
    "    \n",
    "    # removes categories of descrdiptors from the dataset\n",
    "    bas = data_BAS(df_time, key, dim_remove=dim_remove)\n",
    "\n",
    "    # filters out alarms\n",
    "    if include_alarms is False:\n",
    "        bas1 = alarm_filter(bas, key)\n",
    "    else:\n",
    "        bas1 = bas\n",
    "\n",
    "    # verification statement\n",
    "    \n",
    "    print(\n",
    "        'Time filtered data contains ' + str(df_time.shape[0]) + ' points and '\n",
    "        + str(df_time.shape[1]) + ' dimensions.'\n",
    "    )\n",
    "    print(\n",
    "        'Filtered data contains ' + str(bas1.shape[0]) + ' points and '\n",
    "        + str(bas1.shape[1]) + ' dimensions.'\n",
    "    )\n",
    "\n",
    "    return bas1\n",
    "\n",
    "\n",
    "def data_import(dat_folder, string, keys):\n",
    "    \"\"\"imports plant data and creates data frames with raw data and keys\n",
    "\n",
    "    dat_folder = folder containing raw data.\n",
    "    string = prefix of the csv files to be imported.\n",
    "    keys = file name from current directory containing the keys spreadsheet\n",
    "\n",
    "    Output:\n",
    "    df = dataframe containing plant data\n",
    "    key = dataframe containing descriptor key\"\"\"\n",
    "\n",
    "    # Assert that dat_folder is .csv\n",
    "\n",
    "    # Assert that string is string type\n",
    "\n",
    "    # extracts file names\n",
    "    dat_list = glob.glob(os.path.join(dat_folder, string))\n",
    "    print(dat_list)\n",
    "\n",
    "    # reads and appends content from file to a data frame\n",
    "    df = pd.DataFrame()\n",
    "    for lst in dat_list:\n",
    "        df_add = pd.read_csv(lst)\n",
    "        df = pd.concat([df, df_add], ignore_index=True)\n",
    "\n",
    "    key = pd.read_excel(keys)\n",
    "\n",
    "    return df, key\n",
    "\n",
    "\n",
    "def data_BAS(df, key, dim_remove=[]):\n",
    "    '''Filters out descriptors containing NaN values, calculated descriptors,\n",
    "     and miscelaneous descriptors specified by the user.\n",
    "\n",
    "    Input:\n",
    "    df = dataframe containing plant data\n",
    "    key = dataframe containing descriptor key\n",
    "    dim_remove = list of descriptors to remove from dataset (default = NULL)\n",
    "\n",
    "    Output:\n",
    "    bas = dataframe filtered for descriptors and NaN values'''\n",
    "\n",
    "    # finds keys from categories BAS, Chiller, Condenser Water Pump\n",
    "    # and Cooling Tower Cell\n",
    "    keys = []\n",
    "\n",
    "    key_list = [\"BAS\", \"Chiller\", \"Condenser Water Pump\", \"Cooling Tower Cell\"]\n",
    "\n",
    "    for k in range(0, len(key_list)):\n",
    "        key_loop = key.loc[\n",
    "            key['PointType'].str.contains(key_list[k])==True, \n",
    "            'DataPointName'\n",
    "        ].T.tolist()\n",
    "        keys += key_loop\n",
    "\n",
    "    # removes DataPointNames that containt the prefix CHWV\n",
    "    kw = [x for x in keys if 'kW' not in x]\n",
    "    vals = [x for x in kw if not x.startswith('CHWV')]\n",
    "\n",
    "    # optional dimension remover\n",
    "    for dim in dim_remove:\n",
    "        vals.remove(dim)\n",
    "\n",
    "    # tests whether all values from the point list spreadsheet are column\n",
    "    # headings of the dataset\n",
    "    print('Descriptors in the points list that are not in the datasets.')\n",
    "    for x in vals:\n",
    "        if x not in df.columns:\n",
    "            # prints and removes any string not found in the data\n",
    "            print(x)\n",
    "            vals.remove(x)\n",
    "    # tests whether all values from the point list spreadsheet are column\n",
    "    # headings of the dataset\n",
    "\n",
    "    vals_new = [x for x in vals if x in df.columns]\n",
    "\n",
    "    # expresses data using columns specified by the vals list\n",
    "    bas = df[vals_new+['OptimumControl', 'kW/Ton']]\n",
    "\n",
    "    print(\n",
    "        'Original data contains ' + str(df.shape[0]) + ' points and '\n",
    "        + str(df.shape[1]) + ' dimensions.'\n",
    "    )\n",
    "\n",
    "    return bas.dropna()\n",
    "\n",
    "\n",
    "def time_filter(df, start, end):\n",
    "    ''' Filters out a specified timestamp from the dataset \n",
    "    \n",
    "    df = dataframe containing the plant data\n",
    "    start = starting date\n",
    "    end = end date'''\n",
    "    \n",
    "    time_list = []\n",
    "    date1 = '2017-06-06'\n",
    "    date2 = '2017-06-20'\n",
    "    start = datetime.datetime.strptime(date1, '%Y-%m-%d')\n",
    "    end = datetime.datetime.strptime(date2, '%Y-%m-%d')\n",
    "    step = datetime.timedelta(days=1)\n",
    "    while start <= end:\n",
    "        start += step\n",
    "        time_list.append(str(start.date()))\n",
    "        \n",
    "    df_time = df[\n",
    "        ~df['timestamp'].str.contains('|'.join(time_list))\n",
    "    ]\n",
    "    \n",
    "    return df_time\n",
    "\n",
    "\n",
    "def alarm_filter(bas, key):\n",
    "    \"\"\"removes any datapoints with alarms going off or without optimum control\n",
    "\n",
    "    bas = dataframe containing plant data\n",
    "    key = dataframe containing descriptor key\"\"\"\n",
    "\n",
    "    # filters kes to select those with alarm units that are also BAS\n",
    "    key_alarm = key.loc[\n",
    "        key['Units'].str.contains(\"Normal/Alarm\") == True, 'DataPointName'\n",
    "    ]\n",
    "\n",
    "    vals = [x for x in key_alarm if x in bas.columns]\n",
    "\n",
    "    # tests whether an alarm is going off\n",
    "    bas_start = bas.shape[0]\n",
    "    for alm in vals:\n",
    "\n",
    "        # returns dataframe that have no alarms going off\n",
    "        bas = bas[bas[alm] == 0]\n",
    "\n",
    "        bas_end = bas.shape[0]\n",
    "\n",
    "        # compares shape before and after alarm filtering\n",
    "        if bas_end != bas_start:\n",
    "            print(\n",
    "                'A ' + alm + ' was noted and ' + str(bas_start-bas_end)\n",
    "                + ' datapoints were removed from the dataset.'\n",
    "            )\n",
    "\n",
    "    bas = bas[bas['OptimumControl'] == 1]\n",
    "\n",
    "    return bas\n",
    "\n",
    "# two plant test train function\n",
    "# THIS FUNCTION DOES NOT NEED UNIT TESTS!!!!\n",
    "# IT IS NOT BEING USED RIGHT NOW!!!\n",
    "\n",
    "\n",
    "def train_plt_ref(\n",
    "    train_folder, train_string, train_keys, test_folder,\n",
    "    test_string, test_keys, include_alarms=True\n",
    "):\n",
    "    \"\"\"imports test and train plant data and creates data frames with\n",
    "     filtered data\n",
    "\n",
    "    Input:\n",
    "    train_folder = folder containing raw data.\n",
    "    train_string = prefix of the csv files to be imported.\n",
    "    train_keys = file name from current directory containing the keys\n",
    "     spreadsheet\n",
    "    test_folder = folder containing raw data.\n",
    "    test_string = prefix of the csv files to be imported.\n",
    "    test_keys = file name from current directory containing keys spreadsheet\n",
    "    include_alarms = include or remove alarms from dataset (default = True)\n",
    "\n",
    "    Output:\n",
    "    df_bas1_train = dataframe containing filtered training plant data\n",
    "    df_ce bas1_test = dataframe containing filtered test plant data\"\"\"\n",
    "    print('Filtering Training Set')\n",
    "    df, key = data_import(train_folder, train_string, train_keys)\n",
    "    bas = data_BAS(df, key)\n",
    "    if include_alarm is False:\n",
    "        bas1_train = alarm_filter(bas, key)\n",
    "    else:\n",
    "        bas1_train = bas\n",
    "\n",
    "    print('Filtering Test Set')\n",
    "    df, key = data_import(test_folder, test_string, test_keys)\n",
    "    bas = data_BAS(df, key)\n",
    "    if include_alarm is False:\n",
    "        bas1 = alarm_filter(bas, key)\n",
    "    else:\n",
    "        bas1 = bas\n",
    "\n",
    "    vals_test = [x for x in bas1.columns if x in bas1_train.columns]\n",
    "    df_bas1_test = bas1[vals_test]\n",
    "    df_bas1_train = bas1_train[vals_test]\n",
    "\n",
    "    return df_bas1_train, df_bas1_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Training Set\n",
      "['../../Plt1\\\\Plt1 m 2017-06.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-07.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-08.csv']\n",
      "Descriptors in the points list that are not in the datasets.\n",
      "CommunicationFailure_COV\n",
      "CH3COM1F\n",
      "CH3Ready\n",
      "CH4COM1F\n",
      "CH4Ready\n",
      "CH4SURGE\n",
      "CH5COM1F\n",
      "CH5Ready\n",
      "Original data contains 22179 points and 413 dimensions.\n",
      "Time filtered data contains 22179 points and 413 dimensions.\n",
      "Filtered data contains 21458 points and 193 dimensions.\n",
      "Filtering Test Set\n",
      "['../../Plt1\\\\Plt1 m 2017-09.csv']\n",
      "['../../Plt1\\\\Plt1 m 2017-10.csv']\n",
      "[]\n",
      "['../../Plt1\\\\Plt1 m 2017-12.csv']\n",
      "['../../Plt1\\\\Plt1 m 2018-01.csv']\n",
      "['../../Plt1\\\\Plt1 m 2018-02.csv']\n",
      "['../../Plt1\\\\Plt1 m 2018-03.csv']\n",
      "Descriptors in the points list that are not in the datasets.\n",
      "CommunicationFailure_COV\n",
      "CH3COM1F\n",
      "CH3Ready\n",
      "CH4COM1F\n",
      "CH4Ready\n",
      "CH4SURGE\n",
      "CH5COM1F\n",
      "CH5Ready\n",
      "Original data contains 51891 points and 414 dimensions.\n",
      "Time filtered data contains 51891 points and 414 dimensions.\n",
      "Filtered data contains 50026 points and 193 dimensions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50026, 193)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_train = ['Plt1 m 2017-06.csv', 'Plt1 m 2017-07.csv', 'Plt1 m 2017-08.csv']\n",
    "lst_test = [\n",
    "    'Plt1 m 2017-09.csv', 'Plt1 m 2017-10.csv', 'Plt1 m 2017-11.csv',\n",
    "    'Plt1 m 2017-12.csv', 'Plt1 m 2018-01.csv', 'Plt1 m 2018-02.csv', 'Plt1 m 2018-03.csv'\n",
    "]\n",
    "\n",
    "df_train, df_test = train_single_plt(\n",
    "    '../../Plt1', lst_train, lst_test, '../../Plt1/Plt1 Points List.xlsx',\n",
    "    '2017-06-06', '2017-06-20', include_alarms=True\n",
    ")\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
